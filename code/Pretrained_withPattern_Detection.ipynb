{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cb4espuLKJiA"
   },
   "source": [
    "##### Copyright 2020 The TensorFlow Hub Authors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2023-02-16T12:23:15.026402Z",
     "iopub.status.busy": "2023-02-16T12:23:15.026154Z",
     "iopub.status.idle": "2023-02-16T12:23:15.030133Z",
     "shell.execute_reply": "2023-02-16T12:23:15.029558Z"
    },
    "id": "jM3hCI1UUzar"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_NEJlxKKjyI"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/text/tutorials/classify_text_with_bert\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/text/docs/tutorials/classify_text_with_bert.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://tfhub.dev/google/collections/bert/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\" />See TF Hub model</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCjmX4zTCkRK"
   },
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dependency of the preprocessing for BERT inputs\n",
    "!pip install -q -U \"tensorflow-text==2.11.*\"\n",
    "!pip install -q tf-models-official==2.11.0\n",
    "!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T12:23:50.844664Z",
     "iopub.status.busy": "2023-02-16T12:23:50.844049Z",
     "iopub.status.idle": "2023-02-16T12:23:53.324655Z",
     "shell.execute_reply": "2023-02-16T12:23:53.323950Z"
    },
    "id": "_XgTpm9ZxoN9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.metrics import f1_score as f1\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 16\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restructure input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(sentence):\n",
    "    return ''.join(char for char in sentence if ord(char) < 128)\n",
    "    \n",
    "def html_Filter(sentence):\n",
    "    sentence = BeautifulSoup(sentence, \"lxml\").text\n",
    "    #print(\"after html_Filter\",sentence)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_files_in_directory(directory_path):\n",
    "    try:\n",
    "        files = os.listdir(directory_path)\n",
    "        for file in files:\n",
    "            file_path = os.path.join(directory_path, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "        print(\"All files deleted successfully.\")\n",
    "    except OSError:\n",
    "        print(\"Error occurred while deleting files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(class_list,file_path,df_data,col = \"y_ETD\"): \n",
    "    for name in class_list:\n",
    "        newpath = file_path+name+'/' \n",
    "        if not os.path.exists(newpath):\n",
    "            os.makedirs(newpath)\n",
    "        else:\n",
    "            delete_files_in_directory(newpath)\n",
    "    \n",
    "#     df_data[col] = list(1-df_data[col].values)\n",
    "    \n",
    "    for i, (sentence, label) in enumerate(zip(df_data[\"issue_clean\"],df_data[col])):\n",
    "        sentence = html_Filter(sentence)\n",
    "        sentence = remove_non_ascii(sentence)\n",
    "    \n",
    "        #vlabel_list.append(label)\n",
    "    \n",
    "        with open(file_path+str(label)+'/'+str(i)+\".txt\",\"w\",encoding=\"UTF-8\") as f:\n",
    "            f.write(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfdata_unzip(\n",
    "    tfdata: tf.data.Dataset,\n",
    "    *,\n",
    "    recursive: bool=False,\n",
    "    eager_numpy: bool=False,\n",
    "    num_parallel_calls: int=tf.data.AUTOTUNE,\n",
    "):\n",
    "    \"\"\"\n",
    "    Unzip a zipped tf.data pipeline.\n",
    "\n",
    "    Args:\n",
    "        tfdata: the :py:class:`tf.data.Dataset`\n",
    "            to unzip.\n",
    "\n",
    "        recursive: Set to ``True`` to recursively unzip\n",
    "            multiple layers of zipped pipelines.\n",
    "            Defaults to ``False``.\n",
    "\n",
    "        eager_numpy: Set this to ``True`` to return\n",
    "            Python lists of primitive types or\n",
    "            :py:class:`numpy.array` objects. Defaults\n",
    "            to ``False``.\n",
    "\n",
    "        num_parallel_calls: The level of parallelism to\n",
    "            each time we ``map()`` over a\n",
    "            :py:class:`tf.data.Dataset`.\n",
    "\n",
    "    Returns:\n",
    "        Returns a Python list of either\n",
    "             :py:class:`tf.data.Dataset` or NumPy\n",
    "             arrays.\n",
    "    \"\"\"\n",
    "    if isinstance(tfdata.element_spec, tf.TensorSpec):\n",
    "        if eager_numpy:\n",
    "            return list(tfdata.as_numpy_iterator())\n",
    "        return tfdata\n",
    "        \n",
    "    \n",
    "    def tfdata_map(i: int) -> list:\n",
    "        return tfdata.map(\n",
    "            lambda *cols: cols[i],\n",
    "            deterministic=True,\n",
    "            num_parallel_calls=num_parallel_calls,\n",
    "        )\n",
    "\n",
    "    if isinstance(tfdata.element_spec, tuple):\n",
    "        num_columns = len(tfdata.element_spec)\n",
    "        if recursive:\n",
    "            return [\n",
    "                tfdata_unzip(\n",
    "                    tfdata_map(i),\n",
    "                    recursive=recursive,\n",
    "                    eager_numpy=eager_numpy,\n",
    "                    num_parallel_calls=num_parallel_calls,\n",
    "                )\n",
    "                for i in range(num_columns)\n",
    "            ]\n",
    "        else:\n",
    "            return [\n",
    "                tfdata_map(i)\n",
    "                for i in range(num_columns)\n",
    "            ]\n",
    "\n",
    "    raise ValueError(\n",
    "        \"Unknown tf.data.Dataset element_spec: \" +\n",
    "        str(tfdata.element_spec)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Generation for 10-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_etd_list = []\n",
    "val_ds_etd_list = []\n",
    "test_ds_etd_list = []\n",
    "\n",
    "train_ds_ps_list = []\n",
    "val_ds_ps_list = []\n",
    "test_ds_ps_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"../data/cross_validate_10/\"\n",
    "dir_list = sorted(os.listdir(path))\n",
    "\n",
    "for folder in dir_list:\n",
    "    train_etd_path = path+folder+\"/train/train_bert_etd/\"\n",
    "    val_etd_path = path+folder+\"/val/val_bert_etd/\"\n",
    "    test_etd_path = path+folder+\"/test/test_bert_etd/\"\n",
    "    \n",
    "    train_ps_path = path+folder+\"/train/train_bert_ps/\"\n",
    "    val_ps_path = path+folder+\"/val/val_bert_ps/\"\n",
    "    test_ps_path = path+folder+\"/test/test_bert_ps/\"\n",
    "    \n",
    "    df_train = pd.read_csv(path+folder+\"/train.csv\")\n",
    "    df_val = pd.read_csv(path+folder+\"/val.csv\")\n",
    "    df_test = pd.read_csv(path+folder+\"/test.csv\")\n",
    "    \n",
    "    df_etd_pattern_train = pd.read_csv(path+folder+\"/train/etd_pattern_train.csv\")\n",
    "    df_ps_pattern_train = pd.read_csv(path+folder+\"/train/ps_pattern_train.csv\")\n",
    "    df_etd_pattern_test = pd.read_csv(path+folder+\"/test/etd_pattern_test.csv\")\n",
    "    df_ps_pattern_test = pd.read_csv(path+folder+\"/test/ps_pattern_test.csv\")\n",
    "    df_etd_pattern_val = pd.read_csv(path+folder+\"/val/etd_pattern_val.csv\")\n",
    "    df_ps_pattern_val = pd.read_csv(path+folder+\"/val/ps_pattern_val.csv\")\n",
    "    \n",
    "    class_list = sorted([str(folder) for folder in set(df_train[\"y_ETD\"])])\n",
    "    \n",
    "#     create_folder(class_list,train_etd_path,df_train,col = \"y_ETD\")\n",
    "#     create_folder(class_list,val_etd_path,df_val,col = \"y_ETD\")\n",
    "#     create_folder(class_list,test_etd_path,df_test,col = \"y_ETD\")\n",
    "    \n",
    "#     create_folder(class_list,train_ps_path,df_train,col = \"y_PS\")\n",
    "#     create_folder(class_list,val_ps_path,df_val,col = \"y_PS\")\n",
    "#     create_folder(class_list,test_ps_path,df_test,col = \"y_PS\")\n",
    "    \n",
    "    X_etd_train = df_etd_pattern_train.values\n",
    "    X_ps_train = df_ps_pattern_train.values\n",
    "    X_etd_test = df_etd_pattern_test.values\n",
    "    X_ps_test = df_ps_pattern_test.values\n",
    "    X_etd_val = df_etd_pattern_val.values\n",
    "    X_ps_val = df_ps_pattern_val.values\n",
    "    \n",
    "    #For ETD\n",
    "    pattern_train_ds = tf.data.Dataset.from_tensor_slices(X_etd_train)\n",
    "    pattern_val_ds = tf.data.Dataset.from_tensor_slices(X_etd_val)\n",
    "    pattern_test_ds = tf.data.Dataset.from_tensor_slices(X_etd_test)\n",
    "    \n",
    "    train_ds_etd = tf.keras.utils.text_dataset_from_directory(\n",
    "        train_etd_path,\n",
    "        labels='inferred',\n",
    "        label_mode = 'categorical',\n",
    "        batch_size=batch_size,\n",
    "        shuffle = False,\n",
    "        seed=seed)\n",
    "\n",
    "    # use tfdata_unzip() to separate input from labels\n",
    "    input_train, input_labels = tfdata_unzip(train_ds_etd)\n",
    "    model_inputs = tf.data.Dataset.zip(({\"sentences\":input_train.unbatch(), \"patterns\":pattern_train_ds}, input_labels.unbatch()))\n",
    "    model_inputs = model_inputs.shuffle(len(df_train), reshuffle_each_iteration=False).batch(batch_size = batch_size).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    val_ds_etd = tf.keras.utils.text_dataset_from_directory(\n",
    "        val_etd_path,\n",
    "        labels='inferred',\n",
    "        label_mode = 'categorical',\n",
    "        batch_size=batch_size,\n",
    "        shuffle = False,\n",
    "        seed=seed)\n",
    "\n",
    "    # use tfdata_unzip() to separate input from labels\n",
    "    input_val, input_val_labels = tfdata_unzip(val_ds_etd)\n",
    "    model_val = tf.data.Dataset.zip(({\"sentences\":input_val.unbatch(), \"patterns\":pattern_val_ds}, input_val_labels.unbatch()))\n",
    "    model_val = model_val.shuffle(len(df_val), reshuffle_each_iteration=False).batch(batch_size = batch_size).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    test_ds_etd = tf.keras.utils.text_dataset_from_directory(\n",
    "        test_etd_path,\n",
    "        label_mode = 'categorical',\n",
    "        shuffle = False,\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    # use tfdata_unzip() to separate input from labels\n",
    "    input_test, input_test_labels = tfdata_unzip(test_ds_etd)\n",
    "    model_test = tf.data.Dataset.zip(({\"sentences\":input_test.unbatch(), \"patterns\":pattern_test_ds}, input_test_labels.unbatch()))\n",
    "    model_test = model_test.batch(batch_size = batch_size).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    train_ds_etd_list.append(model_inputs)\n",
    "    val_ds_etd_list.append(model_val)\n",
    "    test_ds_etd_list.append(model_test)\n",
    "    \n",
    "     #For PS\n",
    "    pattern_train_ds = tf.data.Dataset.from_tensor_slices(X_ps_train)\n",
    "    pattern_val_ds = tf.data.Dataset.from_tensor_slices(X_ps_val)\n",
    "    pattern_test_ds = tf.data.Dataset.from_tensor_slices(X_ps_test)\n",
    "    \n",
    "    train_ds_ps = tf.keras.utils.text_dataset_from_directory(\n",
    "        train_ps_path,\n",
    "        labels='inferred',\n",
    "        label_mode = 'categorical',\n",
    "        batch_size=batch_size,\n",
    "        shuffle = False,\n",
    "        seed=seed)\n",
    "    \n",
    "    # use tfdata_unzip() to separate input from labels\n",
    "    input_train, input_labels = tfdata_unzip(train_ds_ps)\n",
    "    model_inputs = tf.data.Dataset.zip(({\"sentences\":input_train.unbatch(), \"patterns\":pattern_train_ds}, input_labels.unbatch()))\n",
    "    model_inputs = model_inputs.shuffle(len(df_train), reshuffle_each_iteration=False).batch(batch_size = batch_size).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    val_ds_ps = tf.keras.utils.text_dataset_from_directory(\n",
    "        val_ps_path,\n",
    "        labels='inferred',\n",
    "        label_mode = 'categorical',\n",
    "        batch_size=batch_size,\n",
    "        shuffle = False,\n",
    "        seed=seed)\n",
    "    \n",
    "    # use tfdata_unzip() to separate input from labels\n",
    "    input_val, input_val_labels = tfdata_unzip(val_ds_ps)\n",
    "    model_val = tf.data.Dataset.zip(({\"sentences\":input_val.unbatch(), \"patterns\":pattern_val_ds}, input_val_labels.unbatch()))\n",
    "    model_val = model_val.shuffle(len(df_val), reshuffle_each_iteration=False).batch(batch_size = batch_size).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    test_ds_ps = tf.keras.utils.text_dataset_from_directory(\n",
    "        test_ps_path,\n",
    "        label_mode = 'categorical',\n",
    "        shuffle = False,\n",
    "        batch_size=batch_size)\n",
    "    \n",
    "    # use tfdata_unzip() to separate input from labels\n",
    "    input_test, input_test_labels = tfdata_unzip(test_ds_ps)\n",
    "    model_test = tf.data.Dataset.zip(({\"sentences\":input_test.unbatch(), \"patterns\":pattern_test_ds}, input_test_labels.unbatch()))\n",
    "    model_test = model_test.batch(batch_size = batch_size).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    train_ds_ps_list.append(model_inputs)\n",
    "    val_ds_ps_list.append(model_val)\n",
    "    test_ds_ps_list.append(model_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dX8FtlpGJRE6"
   },
   "source": [
    "## Loading models from TensorFlow Hub\n",
    "\n",
    "Here you can choose which BERT model you will load from TensorFlow Hub and fine-tune. There are multiple BERT models available.\n",
    "\n",
    "  - [BERT-Base](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3), [Uncased](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3) and [seven more models](https://tfhub.dev/google/collections/bert/1) with trained weights released by the original BERT authors.\n",
    "  - [Small BERTs](https://tfhub.dev/google/collections/bert/1) have the same general architecture but fewer and/or smaller Transformer blocks, which lets you explore tradeoffs between speed, size and quality.\n",
    "  - [ALBERT](https://tfhub.dev/google/collections/albert/1): four different sizes of \"A Lite BERT\" that reduces model size (but not computation time) by sharing parameters between layers.\n",
    "  - [BERT Experts](https://tfhub.dev/google/collections/experts/bert/1): eight models that all have the BERT-base architecture but offer a choice between different pre-training domains, to align more closely with the target task.\n",
    "  - [Electra](https://tfhub.dev/google/collections/electra/1) has the same architecture as BERT (in three different sizes), but gets pre-trained as a discriminator in a set-up that resembles a Generative Adversarial Network (GAN).\n",
    "  - BERT with Talking-Heads Attention and Gated GELU [[base](https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1), [large](https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_large/1)] has two improvements to the core of the Transformer architecture.\n",
    "\n",
    "The model documentation on TensorFlow Hub has more details and references to the\n",
    "research literature. Follow the links above, or click on the [`tfhub.dev`](http://tfhub.dev) URL\n",
    "printed after the next cell execution.\n",
    "\n",
    "The suggestion is to start with a Small BERT (with fewer parameters) since they are faster to fine-tune. If you like a small model but with higher accuracy, ALBERT might be your next option. If you want even better accuracy, choose\n",
    "one of the classic BERT sizes or their recent refinements like Electra, Talking Heads, or a BERT Expert.\n",
    "\n",
    "Aside from the models available below, there are [multiple versions](https://tfhub.dev/google/collections/transformer_encoders_text/1) of the models that are larger and can yield even better accuracy, but they are too big to be fine-tuned on a single GPU. You will be able to do that on the [Solve GLUE tasks using BERT on a TPU colab](https://www.tensorflow.org/text/tutorials/bert_glue).\n",
    "\n",
    "You'll see in the code below that switching the tfhub.dev URL is enough to try any of these models, because all the differences between them are encapsulated in the SavedModels from TF Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2023-02-16T12:24:15.254681Z",
     "iopub.status.busy": "2023-02-16T12:24:15.254445Z",
     "iopub.status.idle": "2023-02-16T12:24:15.264457Z",
     "shell.execute_reply": "2023-02-16T12:24:15.263826Z"
    },
    "id": "y8_ctG55-uTX"
   },
   "outputs": [],
   "source": [
    "#@title Choose a BERT model to fine-tune\n",
    "\n",
    "bert_model_name = 'small_bert/bert_en_uncased_L-12_H-768_A-12'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'electra_large':\n",
    "        'https://tfhub.dev/google/electra_large/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "    'roberta_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/jeongukjae/roberta_en_cased_L-12_H-768_A-12/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_large':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'roberta_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDNKfAXbDnJH"
   },
   "source": [
    "## Define custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_combined_model(feature_size, dropout = 0.2, num_class=2):\n",
    "    input1 = tf.keras.layers.Input(shape=(), dtype=tf.string, name='sentences')\n",
    "    input2 = tf.keras.layers.Input(shape=(feature_size,), dtype=tf.float32, name='patterns')\n",
    "    \n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(input1)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.concat([net, input2], 1)\n",
    "    net = tf.keras.layers.Dropout(dropout)(net)\n",
    "    \n",
    "    net = tf.keras.layers.Dense(num_class, activation='softmax', name='classifier')(net)\n",
    "    #return tf.keras.Model([input1,input2], net)\n",
    "    return tf.keras.Model(inputs={'sentences': input1, 'patterns': input2}, outputs=net)\n",
    "    #return tf.keras.Model(input1, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbUWoZMwc302"
   },
   "source": [
    "## Model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T12:24:28.422218Z",
     "iopub.status.busy": "2023-02-16T12:24:28.421971Z",
     "iopub.status.idle": "2023-02-16T12:24:28.435227Z",
     "shell.execute_reply": "2023-02-16T12:24:28.434604Z"
    },
    "id": "OWPOZE-L3AgE"
   },
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "metrics = tfa.metrics.F1Score(num_classes =2, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(ds, epochs):\n",
    "    steps_per_epoch = tf.data.experimental.cardinality(ds).numpy()\n",
    "    num_train_steps = steps_per_epoch * epochs\n",
    "    num_warmup_steps = int(0.1*num_train_steps)\n",
    "    \n",
    "    init_lr = 3e-5\n",
    "    optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=0,\n",
    "                                          optimizer_type='adamw')\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_report(classifier_model,dataset, files, df_data):\n",
    "    y_prob = classifier_model.predict(dataset)\n",
    "    predicted_category_index = [np.argmax(y_prob[i]) for i in range(len(y_prob))]\n",
    "    y_pred = [class_list[index] for index in predicted_category_index]\n",
    "    \n",
    "    ## generate report\n",
    "    label_pred = np.zeros(len(y_pred))\n",
    "\n",
    "    for i, file in enumerate(files):\n",
    "        sid = file[:-4]\n",
    "        label_pred[int(sid)] = y_pred[i]\n",
    "    \n",
    "    return label_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "test_data = []\n",
    "ylabels = []\n",
    "yhatlabels = []\n",
    "dropout_list = np.linspace(0.2, 0.6, 5)\n",
    "epochs = 10\n",
    "\n",
    "for i,(train_ds,val_ds,test_ds) in enumerate(zip(train_ds_etd_list, val_ds_etd_list, test_ds_etd_list)):\n",
    "    optimizer = create_optimizer(train_ds,epochs)\n",
    "    max_clf = build_classifier_model(dropout = 0.2)\n",
    "    max_clf.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)\n",
    "    \n",
    "    print(f'Training model with {tfhub_handle_encoder}')\n",
    "    history = max_clf.fit(x=train_ds,\n",
    "                        validation_data=val_ds,\n",
    "                        verbose=0,\n",
    "                        epochs=epochs)\n",
    "    \n",
    "    max_clf = None\n",
    "    max_score = 0\n",
    "    best_dropout = 0.2\n",
    "    print(f'Training model with {tfhub_handle_encoder}')\n",
    "    for rate in dropout_list:\n",
    "        clf = build_classifier_model(dropout = rate)\n",
    "        clf.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)\n",
    "    \n",
    "    \n",
    "        history = clf.fit(x=train_ds,\n",
    "                        validation_data=val_ds,\n",
    "                        verbose=0,\n",
    "                        epochs=epochs)\n",
    "        \n",
    "        curr_score = np.average(history.history[\"val_classifier1_f1_score\"])\n",
    "        if curr_score > max_score:\n",
    "            max_score = curr_score\n",
    "            max_clf = clf\n",
    "            best_dropout = rate\n",
    "    \n",
    "    print(\"fold\",(i+1),\": best dropout rate is\",best_dropout)\n",
    "    test_etd_path = path+str(i)+\"/test/test_bert_etd/\"\n",
    "    df_test = pd.read_csv(path+str(i)+\"/test.csv\")\n",
    "    test_files = []\n",
    "    for test_folder in class_list:\n",
    "        file_path = test_etd_path+test_folder+\"/\"\n",
    "        test_files.extend(sorted(os.listdir(file_path)))\n",
    "    \n",
    "    label_pred = evaluation_report(max_clf, test_ds,test_files,df_test)\n",
    "    \n",
    "    y_test = df_test[\"y_ETD\"].values\n",
    "    precison = mt.precision_score(y_test, label_pred)\n",
    "    recall = mt.recall_score(y_test, label_pred)\n",
    "    score = mt.f1_score(y_test, label_pred)\n",
    "    \n",
    "    test_data.extend(list(df_test[\"issue_clean\"].values))\n",
    "    ylabels.extend(list(y_test))\n",
    "    yhatlabels.extend(list(label_pred))\n",
    "    \n",
    "    precision_list.append(precison)\n",
    "    recall_list.append(recall)\n",
    "    f1_list.append(score)\n",
    "\n",
    "print(\"precision:\",round(np.average(precision_list),3),\"recall:\",round(np.average(recall_list),3),\"F1:\",round(np.average(f1_list),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame()\n",
    "df_final[\"issue\"] = test_data\n",
    "df_final[\"y\"] = ylabels\n",
    "df_final[\"y_pred\"] = yhatlabels\n",
    "df_final\n",
    "df_final.to_csv(\"../data/BERT_ETD_pattern_result.csv\",index =None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "test_data = []\n",
    "ylabels = []\n",
    "yhatlabels = []\n",
    "dropout_list = np.linspace(0.2, 0.6, 5)\n",
    "epochs = 10\n",
    "    \n",
    "for i,(train_ds,val_ds,test_ds) in enumerate(zip(train_ds_ps_list, val_ds_ps_list, test_ds_ps_list)):\n",
    "    optimizer = create_optimizer(train_ds,epochs) \n",
    "    max_clf = build_classifier_model(dropout = 0.2)\n",
    "    max_clf.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)\n",
    "    \n",
    "    print(f'Training model with {tfhub_handle_encoder}')\n",
    "    history = max_clf.fit(x=train_ds,\n",
    "                        validation_data=val_ds,\n",
    "                        verbose=0,\n",
    "                        epochs=epochs)\n",
    "    max_clf = None\n",
    "    max_score = 0\n",
    "    best_dropout = 0.2\n",
    "    print(f'Training model with {tfhub_handle_encoder}')\n",
    "    for rate in dropout_list:\n",
    "        clf = build_classifier_model(dropout = rate)\n",
    "        clf.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)\n",
    "    \n",
    "    \n",
    "        history = clf.fit(x=train_ds,\n",
    "                        validation_data=val_ds,\n",
    "                        verbose=0,\n",
    "                        epochs=epochs)\n",
    "        \n",
    "        curr_score = np.average(history.history[\"val_classifier1_f1_score\"])\n",
    "        if curr_score > max_score:\n",
    "            max_score = curr_score\n",
    "            max_clf = clf\n",
    "            best_dropout = rate\n",
    "    \n",
    "    print(\"fold\",(i+1),\": best dropout rate is\",best_dropout)\n",
    "    test_etd_path = path+str(i)+\"/test/test_bert_ps/\"\n",
    "    df_test = pd.read_csv(path+str(i)+\"/test.csv\")\n",
    "    test_files = []\n",
    "    for test_folder in class_list:\n",
    "        file_path = test_etd_path+test_folder+\"/\"\n",
    "        test_files.extend(sorted(os.listdir(file_path)))\n",
    "    \n",
    "    label_pred = evaluation_report(max_clf, test_ds,test_files,df_test)\n",
    "    \n",
    "    y_test = df_test[\"y_PS\"].values\n",
    "    precison = mt.precision_score(y_test, label_pred)\n",
    "    recall = mt.recall_score(y_test, label_pred)\n",
    "    score = mt.f1_score(y_test, label_pred)\n",
    "    \n",
    "    test_data.extend(list(df_test[\"issue_clean\"].values))\n",
    "    ylabels.extend(list(y_test))\n",
    "    yhatlabels.extend(list(label_pred))\n",
    "    \n",
    "    precision_list.append(precison)\n",
    "    recall_list.append(recall)\n",
    "    f1_list.append(score)\n",
    "\n",
    "print(\"precision:\",round(np.average(precision_list),3),\"recall:\",round(np.average(recall_list),3),\"F1:\",round(np.average(f1_list),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame()\n",
    "df_final[\"issue\"] = test_data\n",
    "df_final[\"y\"] = ylabels\n",
    "df_final[\"y_pred\"] = yhatlabels\n",
    "df_final\n",
    "df_final.to_csv(\"../data/pretrained/BERT_PS_pattern_result.csv\",index =None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = 'roberta_en_cased_L-12_H-768_A-12'\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "test_data = []\n",
    "ylabels = []\n",
    "yhatlabels = []\n",
    "dropout_list = np.linspace(0.2, 0.6, 5)\n",
    "epochs = 10\n",
    "\n",
    "for i,(train_ds,val_ds,test_ds) in enumerate(zip(train_ds_etd_list, val_ds_etd_list, test_ds_etd_list)):\n",
    "    optimizer = create_optimizer(train_ds,epochs)\n",
    "    max_clf = build_classifier_model(dropout = 0.2)\n",
    "    max_clf.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)\n",
    "    \n",
    "    print(f'Training model with {tfhub_handle_encoder}')\n",
    "    history = max_clf.fit(x=train_ds,\n",
    "                        validation_data=val_ds,\n",
    "                        verbose=0,\n",
    "                        epochs=epochs)\n",
    "    \n",
    "    max_clf = None\n",
    "    max_score = 0\n",
    "    best_dropout = 0.2\n",
    "    print(f'Training model with {tfhub_handle_encoder}')\n",
    "    for rate in dropout_list:\n",
    "        clf = build_classifier_model(dropout = rate)\n",
    "        clf.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)\n",
    "    \n",
    "    \n",
    "        history = clf.fit(x=train_ds,\n",
    "                        validation_data=val_ds,\n",
    "                        verbose=0,\n",
    "                        epochs=epochs)\n",
    "        \n",
    "        curr_score = np.average(history.history[\"val_classifier1_f1_score\"])\n",
    "        if curr_score > max_score:\n",
    "            max_score = curr_score\n",
    "            max_clf = clf\n",
    "            best_dropout = rate\n",
    "    \n",
    "    print(\"fold\",(i+1),\": best dropout rate is\",best_dropout)\n",
    "    test_etd_path = path+str(i)+\"/test/test_bert_etd/\"\n",
    "    df_test = pd.read_csv(path+str(i)+\"/test.csv\")\n",
    "    test_files = []\n",
    "    for test_folder in class_list:\n",
    "        file_path = test_etd_path+test_folder+\"/\"\n",
    "        test_files.extend(sorted(os.listdir(file_path)))\n",
    "    \n",
    "    label_pred = evaluation_report(max_clf, test_ds,test_files,df_test)\n",
    "    \n",
    "    y_test = df_test[\"y_ETD\"].values\n",
    "    precison = mt.precision_score(y_test, label_pred)\n",
    "    recall = mt.recall_score(y_test, label_pred)\n",
    "    score = mt.f1_score(y_test, label_pred)\n",
    "    \n",
    "    test_data.extend(list(df_test[\"issue_clean\"].values))\n",
    "    ylabels.extend(list(y_test))\n",
    "    yhatlabels.extend(list(label_pred))\n",
    "    \n",
    "    precision_list.append(precison)\n",
    "    recall_list.append(recall)\n",
    "    f1_list.append(score)\n",
    "\n",
    "print(\"precision:\",round(np.average(precision_list),3),\"recall:\",round(np.average(recall_list),3),\"F1:\",round(np.average(f1_list),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame()\n",
    "df_final[\"issue\"] = test_data\n",
    "df_final[\"y\"] = ylabels\n",
    "df_final[\"y_pred\"] = yhatlabels\n",
    "df_final\n",
    "df_final.to_csv(\"../data/pretrained/Roberta_ETD_pattern_result.csv\",index =None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "test_data = []\n",
    "ylabels = []\n",
    "yhatlabels = []\n",
    "dropout_list = np.linspace(0.2, 0.6, 5)\n",
    "epochs = 10\n",
    "    \n",
    "for i,(train_ds,val_ds,test_ds) in enumerate(zip(train_ds_ps_list, val_ds_ps_list, test_ds_ps_list)):\n",
    "    optimizer = create_optimizer(train_ds,epochs)\n",
    "    max_clf = build_classifier_model(dropout = 0.2)\n",
    "    max_clf.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)\n",
    "    \n",
    "    print(f'Training model with {tfhub_handle_encoder}')\n",
    "    history = max_clf.fit(x=train_ds,\n",
    "                        validation_data=val_ds,\n",
    "                        verbose=0,\n",
    "                        epochs=epochs)\n",
    "    max_clf = None\n",
    "    max_score = 0\n",
    "    best_dropout = 0.2\n",
    "    print(f'Training model with {tfhub_handle_encoder}')\n",
    "    for rate in dropout_list:\n",
    "        clf = build_classifier_model(dropout = rate)\n",
    "        clf.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)\n",
    "    \n",
    "    \n",
    "        history = clf.fit(x=train_ds,\n",
    "                        validation_data=val_ds,\n",
    "                        verbose=0,\n",
    "                        epochs=epochs)\n",
    "        \n",
    "        curr_score = np.average(history.history[\"val_classifier1_f1_score\"])\n",
    "        if curr_score > max_score:\n",
    "            max_score = curr_score\n",
    "            max_clf = clf\n",
    "            best_dropout = rate\n",
    "    \n",
    "    print(\"fold\",(i+1),\": best dropout rate is\",best_dropout)\n",
    "    test_etd_path = path+str(i)+\"/test/test_bert_ps/\"\n",
    "    df_test = pd.read_csv(path+str(i)+\"/test.csv\")\n",
    "    test_files = []\n",
    "    for test_folder in class_list:\n",
    "        file_path = test_etd_path+test_folder+\"/\"\n",
    "        test_files.extend(sorted(os.listdir(file_path)))\n",
    "    \n",
    "    label_pred = evaluation_report(max_clf, test_ds,test_files,df_test)\n",
    "    \n",
    "    y_test = df_test[\"y_PS\"].values\n",
    "    precison = mt.precision_score(y_test, label_pred)\n",
    "    recall = mt.recall_score(y_test, label_pred)\n",
    "    score = mt.f1_score(y_test, label_pred)\n",
    "    \n",
    "    test_data.extend(list(df_test[\"issue_clean\"].values))\n",
    "    ylabels.extend(list(y_test))\n",
    "    yhatlabels.extend(list(label_pred))\n",
    "    \n",
    "    precision_list.append(precison)\n",
    "    recall_list.append(recall)\n",
    "    f1_list.append(score)\n",
    "\n",
    "print(\"precision:\",round(np.average(precision_list),3),\"recall:\",round(np.average(recall_list),3),\"F1:\",round(np.average(f1_list),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame()\n",
    "df_final[\"issue\"] = test_data\n",
    "df_final[\"y\"] = ylabels\n",
    "df_final[\"y_pred\"] = yhatlabels\n",
    "df_final\n",
    "df_final.to_csv(\"../data/pretrained/Roberta_PS_pattern_result.csv\",index =None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = 'electra_base'\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "test_data = []\n",
    "ylabels = []\n",
    "yhatlabels = []\n",
    "dropout_list = np.linspace(0.2, 0.6, 5)\n",
    "epochs = 10\n",
    "\n",
    "for i,(train_ds,val_ds,test_ds) in enumerate(zip(train_ds_etd_list, val_ds_etd_list, test_ds_etd_list)):  \n",
    "    optimizer = create_optimizer(train_ds,epochs)\n",
    "    max_clf = build_classifier_model(dropout = 0.2)\n",
    "    max_clf.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)\n",
    "    \n",
    "    print(f'Training model with {tfhub_handle_encoder}')\n",
    "    history = max_clf.fit(x=train_ds,\n",
    "                        validation_data=val_ds,\n",
    "                        verbose=0,\n",
    "                        epochs=epochs)\n",
    "    \n",
    "    max_clf = None\n",
    "    max_score = 0\n",
    "    best_dropout = 0.2\n",
    "    print(f'Training model with {tfhub_handle_encoder}')\n",
    "    for rate in dropout_list:\n",
    "        clf = build_classifier_model(dropout = rate)\n",
    "        clf.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)\n",
    "    \n",
    "    \n",
    "        history = clf.fit(x=train_ds,\n",
    "                        validation_data=val_ds,\n",
    "                        verbose=0,\n",
    "                        epochs=epochs)\n",
    "        \n",
    "        curr_score = np.average(history.history[\"val_classifier1_f1_score\"])\n",
    "        if curr_score > max_score:\n",
    "            max_score = curr_score\n",
    "            max_clf = clf\n",
    "            best_dropout = rate\n",
    "    \n",
    "    print(\"fold\",(i+1),\": best dropout rate is\",best_dropout)\n",
    "    test_etd_path = path+str(i)+\"/test/test_bert_etd/\"\n",
    "    df_test = pd.read_csv(path+str(i)+\"/test.csv\")\n",
    "    test_files = []\n",
    "    for test_folder in class_list:\n",
    "        file_path = test_etd_path+test_folder+\"/\"\n",
    "        test_files.extend(sorted(os.listdir(file_path)))\n",
    "    \n",
    "    label_pred = evaluation_report(max_clf, test_ds,test_files,df_test)\n",
    "    \n",
    "    y_test = df_test[\"y_ETD\"].values\n",
    "    precison = mt.precision_score(y_test, label_pred)\n",
    "    recall = mt.recall_score(y_test, label_pred)\n",
    "    score = mt.f1_score(y_test, label_pred)\n",
    "    \n",
    "    test_data.extend(list(df_test[\"issue_clean\"].values))\n",
    "    ylabels.extend(list(y_test))\n",
    "    yhatlabels.extend(list(label_pred))\n",
    "    \n",
    "    precision_list.append(precison)\n",
    "    recall_list.append(recall)\n",
    "    f1_list.append(score)\n",
    "\n",
    "print(\"precision:\",round(np.average(precision_list),3),\"recall:\",round(np.average(recall_list),3),\"F1:\",round(np.average(f1_list),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame()\n",
    "df_final[\"issue\"] = test_data\n",
    "df_final[\"y\"] = ylabels\n",
    "df_final[\"y_pred\"] = yhatlabels\n",
    "df_final\n",
    "df_final.to_csv(\"../data/pretrained/Electra_ETD_pattern_result.csv\",index =None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "test_data = []\n",
    "ylabels = []\n",
    "yhatlabels = []\n",
    "dropout_list = np.linspace(0.2, 0.6, 5)\n",
    "epochs = 10\n",
    "\n",
    "for i,(train_ds,val_ds,test_ds) in enumerate(zip(train_ds_ps_list, val_ds_ps_list, test_ds_ps_list)):\n",
    "    optimizer = create_optimizer(train_ds,epochs)\n",
    "    max_clf = build_classifier_model(dropout = 0.2)\n",
    "    max_clf.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)\n",
    "    \n",
    "    print(f'Training model with {tfhub_handle_encoder}')\n",
    "    history = max_clf.fit(x=train_ds,\n",
    "                        validation_data=val_ds,\n",
    "                        verbose=0,\n",
    "                        epochs=epochs)\n",
    "    max_clf = None\n",
    "    max_score = 0\n",
    "    best_dropout = 0.2\n",
    "    print(f'Training model with {tfhub_handle_encoder}')\n",
    "    for rate in dropout_list:\n",
    "        clf = build_classifier_model(dropout = rate)\n",
    "        clf.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)\n",
    "    \n",
    "    \n",
    "        history = clf.fit(x=train_ds,\n",
    "                        validation_data=val_ds,\n",
    "                        verbose=0,\n",
    "                        epochs=epochs)\n",
    "        \n",
    "        curr_score = np.average(history.history[\"val_classifier1_f1_score\"])\n",
    "        if curr_score > max_score:\n",
    "            max_score = curr_score\n",
    "            max_clf = clf\n",
    "            best_dropout = rate\n",
    "    \n",
    "    print(\"fold\",(i+1),\": best dropout rate is\",best_dropout)\n",
    "    test_etd_path = path+str(i)+\"/test/test_bert_ps/\"\n",
    "    df_test = pd.read_csv(path+str(i)+\"/test.csv\")\n",
    "    test_files = []\n",
    "    for test_folder in class_list:\n",
    "        file_path = test_etd_path+test_folder+\"/\"\n",
    "        test_files.extend(sorted(os.listdir(file_path)))\n",
    "    \n",
    "    label_pred = evaluation_report(max_clf, test_ds,test_files,df_test)\n",
    "    \n",
    "    y_test = df_test[\"y_PS\"].values\n",
    "    precison = mt.precision_score(y_test, label_pred)\n",
    "    recall = mt.recall_score(y_test, label_pred)\n",
    "    score = mt.f1_score(y_test, label_pred)\n",
    "    \n",
    "    test_data.extend(list(df_test[\"issue_clean\"].values))\n",
    "    ylabels.extend(list(y_test))\n",
    "    yhatlabels.extend(list(label_pred))\n",
    "    \n",
    "    precision_list.append(precison)\n",
    "    recall_list.append(recall)\n",
    "    f1_list.append(score)\n",
    "\n",
    "print(\"precision:\",round(np.average(precision_list),3),\"recall:\",round(np.average(recall_list),3),\"F1:\",round(np.average(f1_list),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame()\n",
    "df_final[\"issue\"] = test_data\n",
    "df_final[\"y\"] = ylabels\n",
    "df_final[\"y_pred\"] = yhatlabels\n",
    "df_final\n",
    "df_final.to_csv(\"../Data/chat_pattern/result/validate_new3/pretrained/Electra_PS_pattern_result.csv\",index =None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uttWpgmSfzq9"
   },
   "source": [
    "### Plot the accuracy and loss over time\n",
    "\n",
    "Based on the `History` object returned by `model.fit()`. You can plot the training and validation loss for comparison, as well as the training and validation accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T12:33:48.495400Z",
     "iopub.status.busy": "2023-02-16T12:33:48.494728Z",
     "iopub.status.idle": "2023-02-16T12:33:48.962741Z",
     "shell.execute_reply": "2023-02-16T12:33:48.962015Z"
    },
    "id": "fiythcODf0xo"
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "acc = history_dict['balanced_accuracy']\n",
    "val_acc = history_dict['val_balanced_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "# r is for \"solid red line\"\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzJZCo-cf-Jf"
   },
   "source": [
    "In this plot, the red lines represent the training loss and accuracy, and the blue lines are the validation loss and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rtn7jewb6dg4"
   },
   "source": [
    "## Export for inference\n",
    "\n",
    "Now you just save your fine-tuned model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T12:33:48.966643Z",
     "iopub.status.busy": "2023-02-16T12:33:48.965985Z",
     "iopub.status.idle": "2023-02-16T12:33:55.303633Z",
     "shell.execute_reply": "2023-02-16T12:33:55.302880Z"
    },
    "id": "ShcvqJAgVera"
   },
   "outputs": [],
   "source": [
    "dataset_name = 'test'\n",
    "saved_model_path = 'sen_bert_m_models/{}_bert'.format(dataset_name.replace('/', '_'))\n",
    "\n",
    "classifier_model.save(saved_model_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbI25bS1vD7s"
   },
   "source": [
    "Let's reload the model, so you can try it side by side with the model that is still in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T12:33:55.307785Z",
     "iopub.status.busy": "2023-02-16T12:33:55.307079Z",
     "iopub.status.idle": "2023-02-16T12:34:01.044422Z",
     "shell.execute_reply": "2023-02-16T12:34:01.043695Z"
    },
    "id": "gUEWVskZjEF0"
   },
   "outputs": [],
   "source": [
    "reloaded_model = tf.saved_model.load(saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyTappHTvNCz"
   },
   "source": [
    "Here you can test your model on any sentence you want, just add to the examples variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T12:34:01.047864Z",
     "iopub.status.busy": "2023-02-16T12:34:01.047626Z",
     "iopub.status.idle": "2023-02-16T12:34:01.446168Z",
     "shell.execute_reply": "2023-02-16T12:34:01.445477Z"
    },
    "id": "VBWzH6exlCPS"
   },
   "outputs": [],
   "source": [
    "def infer_by_cluster(text_list, tag_list):\n",
    "    \n",
    "    text_list = [html_Filter(sentence) for sentence in text_list]\n",
    "    text_list = [remove_non_ascii(sentence) for sentence in text_list]\n",
    "    text = tf.constant(text_list)\n",
    "    \n",
    "    feature = vectorizer.fit_transform(tag_list).toarray()\n",
    "    feature = tf.constant(feature, dtype=tf.float32)\n",
    "    \n",
    "    input_dict = {\n",
    "        \"sentences\": text,\n",
    "        \"tags\": feature\n",
    "    }\n",
    "\n",
    "    y_prob = reloaded_model(input_dict, training=False)\n",
    "\n",
    "    predicted_category_index = [np.argmax(y_prob[i]) for i in range(len(y_prob))]\n",
    "    y_pred = [class_list[index] for index in predicted_category_index]\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "        \"How to convert nested setTimeouts to promises <p>Using rsvp.js or any other promises/A+ implementation how can I turn code like...</p> <pre><code>console.log('step 1'); setTimeout(function() { console.log('step 2'); setTimeout(function() { console.log('step 3'); }, 100); }, 300); </code></pre> <p>into a promises implementation?</p>\",# this is the same sentence tried earlier\n",
    "        \"How to convert a char array to a uint16_t by casting type pointer? <pre><code>char bytes[2]; bytes[0] = 0; //0x00 bytes[1] = 24; //0x18 uint16_t* ptrU16 = (uint16_t*)bytes; // I expect it points to the memory block: 0x18 cout &lt;&lt; *ptrU16 &lt;&lt; endl;  // I expect 24, but it is 6144 </code></pre> <p>What's wrong in my code?</p>\"\n",
    "]\n",
    "    \n",
    "tag_list = [\n",
    "        \"<javascript><promise><rsvp.js><rsvp-promise>\",\n",
    "        \"<c++><casting><uint16>\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = infer_by_cluster(examples,tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "classify_text_with_bert.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
