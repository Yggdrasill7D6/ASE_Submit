{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7684e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.metrics import f1_score as f1\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb1c7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import os\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d42fd1",
   "metadata": {},
   "source": [
    "## Prepare dataset for cross-validation (CV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1437fe43",
   "metadata": {},
   "source": [
    "### Train Test Random Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319b6b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(\"../data/Validation_1000.csv\")\n",
    "df_random = df_all.sample(n=1000,ignore_index=True)\n",
    "df_random.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d9a54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = 10\n",
    "\n",
    "train_path = \"../data/cross_validate_10/\"\n",
    "for i in range(cv):\n",
    "    df_split = np.array_split(df_random, cv)\n",
    "    df_test = df_split[i]\n",
    "    val_i = i-1\n",
    "    if i == 0:\n",
    "        val_i = len(df_split)-1\n",
    "        \n",
    "    df_val = df_split[val_i]\n",
    "    frame_train = [df_split[index] for index in range(len(df_split)) if index != i and index != val_i]\n",
    "    df_train = pd.concat(frame_train)\n",
    "    \n",
    "    if not os.path.exists(train_path+str(i)+\"/\"):\n",
    "        os.makedirs(train_path+str(i)+\"/\")\n",
    "        \n",
    "    df_train.to_csv(train_path+str(i)+\"/\"+\"train.csv\",index = None)\n",
    "    df_val.to_csv(train_path+str(i)+\"/\"+\"val.csv\",index = None)\n",
    "    df_test.to_csv(train_path+str(i)+\"/\"+\"test.csv\",index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da612d10",
   "metadata": {},
   "source": [
    "### Pattern Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d0f700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_matrix(df, col):\n",
    "    pattern_code = set()\n",
    "    for pattern_str in df[col]:\n",
    "        if pattern_str == 0:\n",
    "            continue\n",
    "        patterns = pattern_str.split(\",\")\n",
    "        for pattern in patterns:\n",
    "            if len(pattern) != 0:\n",
    "                pattern_code.add(pattern.strip())\n",
    "                \n",
    "    pattern_list = []\n",
    "    for i in range(len(df)):\n",
    "        pattern_str = df.loc[i][col]\n",
    "        pattern_val = dict.fromkeys(pattern_code,0)\n",
    "        if pattern_str != 0:\n",
    "            patterns = pattern_str.split(\",\")\n",
    "            for pattern in patterns:\n",
    "                if len(pattern) != 0:\n",
    "                    pattern_val[pattern.strip()] += 1\n",
    "    \n",
    "        pattern_list.append(pattern_val)\n",
    "    \n",
    "    \n",
    "    return pattern_code,pd.DataFrame.from_dict(pattern_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09759917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_matrix_testing(pattern_code, df, col):            \n",
    "    pattern_list = []\n",
    "    for i in range(len(df)):\n",
    "        pattern_str = df.loc[i][col]\n",
    "        pattern_val = dict.fromkeys(pattern_code,0)\n",
    "        if pattern_str != 0:\n",
    "            patterns = pattern_str.split(\",\")\n",
    "            for pattern in patterns:\n",
    "                if len(pattern) != 0 and pattern != '0' and pattern in pattern_code:\n",
    "                    pattern_val[pattern.strip()] += 1\n",
    "    \n",
    "        pattern_list.append(pattern_val)\n",
    "    \n",
    "    \n",
    "    return pd.DataFrame.from_dict(pattern_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103d4186",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/PatternDiscovery_1000.csv\")\n",
    "df[\"predicted_ETD\"].fillna(0,inplace=True)\n",
    "df[\"predicted_PS\"].fillna(0,inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60c1f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "etd_pattern_code,df_etd = pattern_matrix(df,\"predicted_ETD\")\n",
    "ps_pattern_code,df_ps = pattern_matrix(df,\"predicted_PS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3997e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for all cross-validate data\n",
    "path = \"../data/cross_validate_10/\"\n",
    "\n",
    "dir_list = os.listdir(path)\n",
    "\n",
    "for folder in dir_list:\n",
    "    df_train = pd.read_csv(path+folder+\"/train.csv\")\n",
    "    df_test = pd.read_csv(path+folder+\"/test.csv\")\n",
    "    df_val = pd.read_csv(path+folder+\"/val.csv\")\n",
    "    \n",
    "    df_train.fillna(0,inplace=True)\n",
    "    df_test.fillna(0,inplace=True)\n",
    "    df_val.fillna(0,inplace=True)\n",
    "    \n",
    "    df_etd_train = pattern_matrix_testing(etd_pattern_code, df_train,\"predicted_ETD\")\n",
    "    df_ps_train = pattern_matrix_testing(ps_pattern_code, df_train,\"predicted_PS\")\n",
    "    df_etd_test = pattern_matrix_testing(etd_pattern_code, df_test,\"predicted_ETD\")\n",
    "    df_ps_test = pattern_matrix_testing(ps_pattern_code, df_test,\"predicted_PS\")\n",
    "    df_etd_val = pattern_matrix_testing(etd_pattern_code, df_val,\"predicted_ETD\")\n",
    "    df_ps_val = pattern_matrix_testing(ps_pattern_code, df_val,\"predicted_PS\")\n",
    "    \n",
    "    if not os.path.exists(path+folder+\"/train/\"):\n",
    "        os.makedirs(path+folder+\"/train/\")\n",
    "    if not os.path.exists(path+folder+\"/test/\"):\n",
    "        os.makedirs(path+folder+\"/test/\")\n",
    "    if not os.path.exists(path+folder+\"/val/\"):\n",
    "        os.makedirs(path+folder+\"/val/\")\n",
    "        \n",
    "    df_etd_train.to_csv(path+folder+\"/train/etd_pattern_train.csv\",index =None)\n",
    "    df_ps_train.to_csv(path+folder+\"/train/ps_pattern_train.csv\",index =None)\n",
    "    df_etd_test.to_csv(path+folder+\"/test/etd_pattern_test.csv\",index =None)\n",
    "    df_ps_test.to_csv(path+folder+\"/test/ps_pattern_test.csv\",index =None)\n",
    "    df_etd_val.to_csv(path+folder+\"/val/etd_pattern_val.csv\",index =None)\n",
    "    df_ps_val.to_csv(path+folder+\"/val/ps_pattern_val.csv\",index =None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced2f37a",
   "metadata": {},
   "source": [
    "### N-gram Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9745296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as py\n",
    "from preprocess_helper import PorterStemmer\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from string import punctuation\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d0c0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasAlphanumeric(term):\n",
    "    for letter in term:\n",
    "        if letter.isalnum():\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77225ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Preprocess:\n",
    "    def __init__(self):\n",
    "        self.stopwords_list = None\n",
    "        self.tag_list = None\n",
    "        self.reservedkeywords = None\n",
    "    \n",
    "    def create_reservedkeywords(self, path, num_words = 100):\n",
    "        df = pd.read_csv(path)\n",
    "        df.sort_values(by=['Count'], ascending=False, inplace=True, ignore_index=True)\n",
    "        self.tag_list = list(df[\"Tag\"].values)\n",
    "        reservedkeywords_extra = [\"c#\",\"f#\",\"c++\",\"node.js\",\"nodejs\",\".json\",\".js\",\".net\",\"objective-c\",\n",
    "                                  \"asp.net\",\"ruby-on-rails\",\"angular.js\"]\n",
    "        self.reservedkeywords = list(df[\"Tag\"].values)[:num_words]\n",
    "        self.reservedkeywords.extend(reservedkeywords_extra)\n",
    "        self.reservedkeywords = set(self.reservedkeywords)\n",
    "        #print(self.reservedkeywords)\n",
    "    \n",
    "    def create_stopwords(self):\n",
    "        self.stopwords_list = stopwords.words('english')\n",
    "        stop_words_extra = [\"i'd\",\"sometime\",\"sometimes\",\"something\",\"someone\",\"somebody\",\"anything\",\"anyone\",\"anybody\",\n",
    "                            \"everytime\",\"everything\",\"everyone\",\"everybody\",\"e.g.\",\"e.g\",\"e.g.,\",\"i.e.\",\"i.e\",\"i.e.,\",\"love\",\n",
    "                            \"know\",\"'s\",\"wonder\"]\n",
    "        self.stopwords_list.extend(stop_words_extra)\n",
    "        stopwords_unsure_list = set(self.stopwords_list).intersection(set(self.tag_list))\n",
    "        self.stopwords_list = set(self.stopwords_list).difference(stopwords_unsure_list)\n",
    "        #print(len(self.stopwords_list),\"stopwords\",self.stopwords_list)\n",
    "    \n",
    "    def remove_non_ascii(self,sentence):\n",
    "        return ''.join(char for char in sentence if ord(char) < 128)\n",
    "    \n",
    "    def html_Filter(self, sentence):\n",
    "        sentence = BeautifulSoup(sentence, \"lxml\").text\n",
    "        #print(\"after html_Filter\",sentence)\n",
    "    \n",
    "        return sentence\n",
    "    \n",
    "    def keywords_transform(self, sentence):\n",
    "        sentence = sentence.lower()\n",
    "        sentence = sentence.replace(\"node js\",\"node.js\")\n",
    "        sentence = sentence.replace(\"objective c\",\"objective-c\")\n",
    "        sentence = sentence.replace(\"ruby on rails\",\"ruby-on-rails\")\n",
    "        sentence = sentence.replace(\"angular js\",\"angular-js\")\n",
    "        \n",
    "        return sentence\n",
    "    \n",
    "    def remove_specialchar(self, sentence, char_to_keep = {}): #'#','+','.','-','\\'','\"',':','?','!',',','_'\n",
    "        punct_set = set(punctuation).difference(char_to_keep)\n",
    "\n",
    "        for i in punct_set:\n",
    "            # Replace the special character with an empty string\n",
    "            sentence=sentence.replace(i,\" \")\n",
    "        \n",
    "        return sentence\n",
    "        \n",
    "    def sentence_stem(self, sentence):\n",
    "        p = PorterStemmer()\n",
    "        output = \"\"\n",
    "\n",
    "        for token in sentence.split(' '):\n",
    "            if token.isalnum():\n",
    "                output += p.stem(token, 0,len(token)-1)+' '\n",
    "            elif token in self.reservedkeywords:\n",
    "                output += token+' '\n",
    "        \n",
    "        #print(\"after sentence_stem\", output.strip())\n",
    "        return output.strip()\n",
    "    \n",
    "\n",
    "    def call(self, dataset = None, keywords = None):\n",
    "        data_clean = []\n",
    "        tag_path = \"../data/tag_dict.csv\"\n",
    "        \n",
    "        #initialize tag_list\n",
    "        if keywords == None:\n",
    "            self.create_reservedkeywords(tag_path)\n",
    "        else:\n",
    "            df = pd.read_csv(tag_path)\n",
    "            self.tag_list = list(df[\"Tag\"].values)\n",
    "            self.reservedkeywords = keywords\n",
    "        \n",
    "        #initialize stopwords\n",
    "        self.create_stopwords()\n",
    "        \n",
    "        for sentence in dataset:\n",
    "            sentence = self.html_Filter(sentence)\n",
    "            sentence = self.remove_non_ascii(sentence)\n",
    "            #sentence = self.keywords_transform(sentence)\n",
    "            sentence = self.remove_specialchar(sentence)\n",
    "            #sentence = self.remove_specialchar(sentence)\n",
    "            sentence = self.sentence_stem(sentence)\n",
    "            \n",
    "            data_clean.append(sentence.strip())\n",
    "            pos.append(posTag.strip())\n",
    "        \n",
    "        return data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f297a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/cross_validate_10/\"\n",
    "data_preprocess = Dataset_Preprocess()\n",
    "dir_list = sorted(os.listdir(path))\n",
    "for folder in dir_list:\n",
    "    df_train = pd.read_csv(path+folder+\"/train.csv\")\n",
    "    df_test = pd.read_csv(path+folder+\"/test.csv\")\n",
    "    df_val = pd.read_csv(path+folder+\"/val.csv\")\n",
    "    \n",
    "    df_combine = pd.concat([df_train,df_val])\n",
    "    issues_clean = data_preprocess.call(dataset=list(df_combine[\"issue_clean\"]))\n",
    "    \n",
    "    issues_clean_train = data_preprocess.call(dataset=list(df_train[\"issue_clean\"]))\n",
    "    issues_clean_test = data_preprocess.call(dataset=list(df_test[\"issue_clean\"]))\n",
    "    issues_clean_val = data_preprocess.call(dataset=list(df_val[\"issue_clean\"]))\n",
    "    \n",
    "    #{1,2,3}-grams\n",
    "    count_vect_all = CountVectorizer(ngram_range = (1,3), binary = True)\n",
    "    X_counts = count_vect_all.fit_transform(issues_clean)\n",
    "    print(X_counts.shape)\n",
    "    \n",
    "    count_vect = CountVectorizer(ngram_range = (1,3), binary = True, vocabulary = count_vect_all.get_feature_names_out())\n",
    "    X_train_counts = count_vect.fit_transform(issues_clean_train)\n",
    "    df_train_ngram = pd.DataFrame(data = X_train_counts.toarray(), columns = count_vect_all.get_feature_names_out())\n",
    "    X_test_counts = count_vect.fit_transform(issues_clean_test)\n",
    "    df_test_ngram = pd.DataFrame(data = X_test_counts.toarray(), columns = count_vect_all.get_feature_names_out())\n",
    "    X_val_counts = count_vect.fit_transform(issues_clean_val)\n",
    "    df_val_ngram = pd.DataFrame(data = X_val_counts.toarray(), columns = count_vect_all.get_feature_names_out())\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(path+folder+\"/train/\"):\n",
    "        os.makedirs(path+folder+\"/train/\")\n",
    "    if not os.path.exists(path+folder+\"/test/\"):\n",
    "        os.makedirs(path+folder+\"/test/\")\n",
    "    if not os.path.exists(path+folder+\"/val/\"):\n",
    "        os.makedirs(path+folder+\"/val/\")\n",
    "        \n",
    "    \n",
    "    df_train_ngram.to_csv(path+folder+\"/train/ngram_train.csv\",index =None)\n",
    "    df_test_ngram.to_csv(path+folder+\"/test/ngram_test.csv\",index =None)\n",
    "    df_val_ngram.to_csv(path+folder+\"/val/ngram_val.csv\",index =None) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f6195c",
   "metadata": {},
   "source": [
    "## Load Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dac4dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_etd_train_list = []\n",
    "X_ps_train_list = []\n",
    "X_ngram_train_list = []\n",
    "X_etd_ngram_train_list = []\n",
    "X_ps_ngram_train_list = []\n",
    "\n",
    "X_etd_test_list = []\n",
    "X_ps_test_list = []\n",
    "X_ngram_test_list = []\n",
    "X_etd_ngram_test_list = []\n",
    "X_ps_ngram_test_list = []\n",
    "\n",
    "X_etd_val_list = []\n",
    "X_ps_val_list = []\n",
    "X_ngram_val_list = []\n",
    "X_etd_ngram_val_list = []\n",
    "X_ps_ngram_val_list = []\n",
    "\n",
    "y_etd_train_list = []\n",
    "y_ps_train_list = []\n",
    "y_etd_test_list = []\n",
    "y_ps_test_list = []\n",
    "y_etd_val_list = []\n",
    "y_ps_val_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc271f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_10fold_data(path):\n",
    "    dir_list = sorted(os.listdir(path))\n",
    "        \n",
    "    for folder in dir_list:\n",
    "        #training files\n",
    "        df_etd_train = pd.read_csv(path+folder+\"/train/etd_pattern_train.csv\")\n",
    "        df_ps_train = pd.read_csv(path+folder+\"/train/ps_pattern_train.csv\")\n",
    "        df_ngram_train = pd.read_csv(path+folder+\"/train/ngram_train.csv\")\n",
    "        df_pos_train = pd.read_csv(path+folder+\"/train/pos_train.csv\")\n",
    "\n",
    "        #test files\n",
    "        df_etd_test = pd.read_csv(path+folder+\"/test/etd_pattern_test.csv\")\n",
    "        df_ps_test = pd.read_csv(path+folder+\"/test/ps_pattern_test.csv\")\n",
    "        df_ngram_test = pd.read_csv(path+folder+\"/test/ngram_test.csv\")\n",
    "        \n",
    "        #validation files\n",
    "        df_etd_val = pd.read_csv(path+folder+\"/val/etd_pattern_val.csv\")\n",
    "        df_ps_val = pd.read_csv(path+folder+\"/val/ps_pattern_val.csv\")\n",
    "        df_ngram_val = pd.read_csv(path+folder+\"/val/ngram_val.csv\")\n",
    "\n",
    "        #label\n",
    "        df_label_train = pd.read_csv(path+folder+\"/train.csv\")\n",
    "        df_label_test = pd.read_csv(path+folder+\"/test.csv\")\n",
    "        df_label_val = pd.read_csv(path+folder+\"/val.csv\")\n",
    "        \n",
    "        #train\n",
    "        X_etd_train = df_etd_train.values\n",
    "        X_ps_train = df_ps_train.values\n",
    "        X_ngram_train = df_ngram_train.values\n",
    "        X_etd_ngram_train = np.hstack((X_etd_train, X_ngram_train))\n",
    "        X_ps_ngram_train = np.hstack((X_ps_train, X_ngram_train))\n",
    "\n",
    "        \n",
    "        #test\n",
    "        X_etd_test = df_etd_test.values\n",
    "        X_ps_test = df_ps_test.values\n",
    "        X_ngram_test = df_ngram_test.values\n",
    "        X_etd_ngram_test = np.hstack((X_etd_test, X_ngram_test))\n",
    "        X_ps_ngram_test = np.hstack((X_ps_test, X_ngram_test))\n",
    "\n",
    "        \n",
    "        #val\n",
    "        X_etd_val = df_etd_val.values\n",
    "        X_ps_val = df_ps_val.values\n",
    "        X_ngram_val = df_ngram_val.values\n",
    "        X_etd_ngram_val = np.hstack((X_etd_val, X_ngram_val))\n",
    "        X_ps_ngram_val = np.hstack((X_ps_val, X_ngram_val))\n",
    "\n",
    "        \n",
    "        y_etd_train = df_label_train[\"y_ETD\"].values\n",
    "        y_ps_train = df_label_train[\"y_PS\"].values\n",
    "        y_etd_test = df_label_test[\"y_ETD\"].values\n",
    "        y_ps_test = df_label_test[\"y_PS\"].values\n",
    "        y_etd_val = df_label_val[\"y_ETD\"].values\n",
    "        y_ps_val = df_label_val[\"y_PS\"].values\n",
    "        \n",
    "        X_etd_train_list.append(X_etd_train)\n",
    "        X_ps_train_list.append(X_ps_train)\n",
    "        X_ngram_train_list.append(X_ngram_train)\n",
    "        X_etd_ngram_train_list.append(X_etd_ngram_train)\n",
    "        X_ps_ngram_train_list.append(X_ps_ngram_train)\n",
    "        \n",
    "        X_etd_test_list.append(X_etd_test)\n",
    "        X_ps_test_list.append(X_ps_test)\n",
    "        X_ngram_test_list.append(X_ngram_test)\n",
    "        X_etd_ngram_test_list.append(X_etd_ngram_test)\n",
    "        X_ps_ngram_test_list.append(X_ps_ngram_test)\n",
    "        \n",
    "        X_etd_val_list.append(X_etd_val)\n",
    "        X_ps_val_list.append(X_ps_val)\n",
    "        X_ngram_val_list.append(X_ngram_val)\n",
    "        X_etd_ngram_val_list.append(X_etd_ngram_val)\n",
    "        X_ps_ngram_val_list.append(X_ps_ngram_val)\n",
    "        \n",
    "        y_etd_train_list.append(y_etd_train)\n",
    "        y_ps_train_list.append(y_ps_train)\n",
    "        y_etd_test_list.append(y_etd_test)\n",
    "        y_ps_test_list.append(y_ps_test)\n",
    "        y_etd_val_list.append(y_etd_val)\n",
    "        y_ps_val_list.append(y_ps_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6d0a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_10fold_data(\"../data/cross_validate_10/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9b9ee8",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e2f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(X_train_list, y_train_list, X_test_list, y_test_list, X_val_list, y_val_list):\n",
    "    param_grid = {'C': np.linspace(0.001, 100, 20)}\n",
    "    yhat = []\n",
    "    ytest = []\n",
    "    clf = None\n",
    "    \n",
    "    for X_train,y_train,X_test,y_test,X_val,y_val in zip(X_train_list, y_train_list, X_test_list, y_test_list, X_val_list, y_val_list):\n",
    "        svc = SVC()\n",
    "        grid_search = GridSearchCV(svc, param_grid, cv=5)\n",
    "        grid_search.fit(X_val, y_val)\n",
    "    \n",
    "        #print('CV Train score: {:.2f}'.format(grid_search.best_score_))\n",
    "        print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "        \n",
    "        clf = SVC(**grid_search.best_params_)\n",
    "        clf.fit(X_train, y_train)\n",
    "        predictions = clf.predict(X_test)\n",
    "        \n",
    "        for val in zip(y_test, predictions):\n",
    "            yhat.append(val[1])\n",
    "            ytest.append(val[0])\n",
    "    \n",
    "    precison = mt.precision_score(ytest, yhat)\n",
    "    recall = mt.recall_score(ytest, yhat)\n",
    "    score = mt.f1_score(ytest, yhat)\n",
    "        \n",
    "    print(\"precision:\",round(precison,3),\"recall:\",round(recall,3),\"F1:\",round(score,3))\n",
    "    \n",
    "    return clf, ytest, yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546c8cac",
   "metadata": {},
   "source": [
    "## Evaluation and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba369470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_save(input_path, yhat_ETD = None, yhat_PS = None):\n",
    "    #merge all test folds\n",
    "    test_list = [pd.read_csv(input_path+folder+\"/test.csv\") for folder in sorted(os.listdir(input_path))]\n",
    "    df_test_all = pd.concat(test_list)\n",
    "    \n",
    "    df_final = pd.DataFrame()\n",
    "    df_final[\"issue\"] = df_test_all[\"issue\"]\n",
    "    ytest = []\n",
    "    yhat = []\n",
    " \n",
    "    if yhat_ETD != None:\n",
    "        df_final[\"predicted_ETD\"] = df_test_all[\"predicted_ETD\"]\n",
    "        df_final[\"y_ETD\"] = df_test_all[\"y_ETD\"]\n",
    "        df_final[\"yhat_ETD\"] = yhat_ETD\n",
    "        \n",
    "        ytest = list(df_final[\"y_ETD\"].values)\n",
    "        yhat = yhat_ETD\n",
    "    if yhat_PS != None:\n",
    "        df_final[\"predicted_PS\"] = df_test_all[\"predicted_PS\"]\n",
    "        df_final[\"y_PS\"] = df_test_all[\"y_PS\"]\n",
    "        df_final[\"yhat_PS\"] = yhat_PS\n",
    "        \n",
    "        ytest = list(df_final[\"y_PS\"].values)\n",
    "        yhat = yhat_PS\n",
    "        \n",
    "    precison = mt.precision_score(ytest, yhat)\n",
    "    recall = mt.recall_score(ytest, yhat)\n",
    "    score = mt.f1_score(ytest, yhat)\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248835a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ETD: pattern\n",
    "clf1,etd_test, etd_hat = cross_validate(X_etd_train_list, y_etd_train_list, X_etd_test_list, y_etd_test_list, X_etd_val_list, y_etd_val_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636a827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../data/cross_validate_10/\"\n",
    "df_final = result_save(train_path, yhat_ETD = etd_hat)\n",
    "df_final.to_csv(\"../experiment/svm/ETD_pattern_result.csv\",index =None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40818a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ETD: n-gram\n",
    "clf3,etd_test3, etd_hat3 = cross_validate(X_ngram_train_list, y_etd_train_list, X_ngram_test_list, y_etd_test_list,X_ngram_val_list, y_etd_val_list)\n",
    "train_path = \"../data/cross_validate_10/\"\n",
    "df_final = result_save(train_path, yhat_ETD = etd_hat3)\n",
    "df_final.to_csv(\"../experiment/svm/ETD_ngram_result.csv\",index =None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7afc0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ETD: pattern + n-gram\n",
    "clf5,etd_test5, etd_hat5 = cross_validate(X_etd_ngram_train_list, y_etd_train_list, X_etd_ngram_test_list, y_etd_test_list, X_etd_ngram_val_list, y_etd_val_list)\n",
    "train_path = \"../data/cross_validate_10/\"\n",
    "df_final = result_save(train_path, yhat_ETD = etd_hat5)\n",
    "df_final.to_csv(\"../experiment/svm/ETD_pattern_ngram_result.csv\",index =None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe1564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. PS: pattern\n",
    "clf8,ps_test, ps_hat =cross_validate(X_ps_train_list, y_ps_train_list, X_ps_test_list, y_ps_test_list, X_ps_val_list, y_ps_val_list)\n",
    "train_path = \"../data/cross_validate_10/\"\n",
    "df_final = result_save(train_path, yhat_PS = ps_hat)\n",
    "df_final.to_csv(\"../experiment/svm/ps_pattern_result.csv\",index =None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3ca512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. PS: n-gram\n",
    "clf10,ps_test3, ps_hat3 = cross_validate(X_ngram_train_list, y_ps_train_list, X_ngram_test_list, y_ps_test_list,X_ngram_val_list, y_ps_val_list)\n",
    "train_path = \"../data/cross_validate_10/\"\n",
    "df_final = result_save(train_path, yhat_PS = ps_hat3)\n",
    "df_final.to_csv(\"../experiment/svm/ps_ngram_result.csv\",index =None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec803b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. PS: pattern + n-gram\n",
    "clf12,ps_test5, ps_hat5 = cross_validate(X_ps_ngram_train_list, y_ps_train_list, X_ps_ngram_test_list, y_ps_test_list,X_ps_ngram_val_list, y_ps_val_list)\n",
    "train_path = \"../data/cross_validate_10/\"\n",
    "df_final = result_save(train_path, yhat_PS = ps_hat5)\n",
    "df_final.to_csv(\"../experiment/svm/ps_pattern_ngram_result.csv\",index =None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54dd8d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea8f090",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
